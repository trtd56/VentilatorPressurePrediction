{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make feature values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(f\"../data/ventilator-pressure-prediction/train.csv\")\n",
    "test_df = pd.read_csv(f\"../data/ventilator-pressure-prediction/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATE_FEATURES = ['R_cate', 'C_cate', 'RC_dot', 'RC_sum']\n",
    "\n",
    "c_dic = {10: 0, 20: 1, 50:2}\n",
    "r_dic = {5: 0, 20: 1, 50:2}\n",
    "rc_sum_dic = {v: i for i, v in enumerate([15, 25, 30, 40, 55, 60, 70, 100])}\n",
    "rc_dot_dic = {v: i for i, v in enumerate([50, 100, 200, 250, 400, 500, 2500, 1000])}    \n",
    "\n",
    "def add_category_features(df):\n",
    "    df['C_cate'] = df['C'].map(c_dic)\n",
    "    df['R_cate'] = df['R'].map(r_dic)\n",
    "    df['RC_sum'] = (df['R'] + df['C']).map(rc_sum_dic)\n",
    "    df['RC_dot'] = (df['R'] * df['C']).map(rc_dot_dic)\n",
    "    return df\n",
    "\n",
    "train_df = add_category_features(train_df)\n",
    "test_df = add_category_features(test_df)\n",
    "\n",
    "train_df[CATE_FEATURES].to_feather(f\"../data/features/train_cate_v1.ftr\")\n",
    "test_df[CATE_FEATURES].to_feather(f\"../data/features/test_cate_v1.ftr\")\n",
    "\n",
    "train_df = train_df.drop(CATE_FEATURES, axis=1)\n",
    "test_df = test_df.drop(CATE_FEATURES, axis=1)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lag Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_LAG = 4\n",
    "LAG_FEATURES = ['breath_time']\n",
    "LAG_FEATURES += [f'u_in_lag_{i}' for i in range(1, USE_LAG+1)]\n",
    "LAG_FEATURES += [f'u_in_time{i}' for i in range(1, USE_LAG+1)]\n",
    "LAG_FEATURES += [f'u_out_lag_{i}' for i in range(1, USE_LAG+1)]\n",
    "\n",
    "def add_lag_feature(df):\n",
    "    # https://www.kaggle.com/kensit/improvement-base-on-tensor-bidirect-lstm-0-173\n",
    "    for lag in range(1, USE_LAG+1):\n",
    "        df[f'breath_id_lag{lag}']=df['breath_id'].shift(lag).fillna(0)\n",
    "        df[f'breath_id_lag{lag}same']=np.select([df[f'breath_id_lag{lag}']==df['breath_id']], [1], 0)\n",
    "\n",
    "        # u_in \n",
    "        df[f'u_in_lag_{lag}'] = df['u_in'].shift(lag).fillna(0) * df[f'breath_id_lag{lag}same']\n",
    "        df[f'u_in_time{lag}'] = df['u_in'] - df[f'u_in_lag_{lag}']\n",
    "        df[f'u_out_lag_{lag}'] = df['u_out'].shift(lag).fillna(0) * df[f'breath_id_lag{lag}same']\n",
    "\n",
    "    # breath_time\n",
    "    df['time_step_lag'] = df['time_step'].shift(1).fillna(0) * df[f'breath_id_lag{lag}same']\n",
    "    df['breath_time'] = df['time_step'] - df['time_step_lag']\n",
    "\n",
    "    drop_columns = ['time_step_lag']\n",
    "    drop_columns += [f'breath_id_lag{i}' for i in range(1, USE_LAG+1)]\n",
    "    drop_columns += [f'breath_id_lag{i}same' for i in range(1, USE_LAG+1)]\n",
    "    df = df.drop(drop_columns, axis=1)\n",
    "\n",
    "    # fill na by zero\n",
    "    df = df.fillna(0)\n",
    "    return df\n",
    "\n",
    "train_df = add_lag_feature(train_df)\n",
    "test_df = add_lag_feature(test_df)\n",
    "\n",
    "train_df[LAG_FEATURES].to_feather(f\"../data/features/train_lag_v1.ftr\")\n",
    "test_df[LAG_FEATURES].to_feather(f\"../data/features/test_lag_v1.ftr\")\n",
    "\n",
    "train_df = train_df.drop(LAG_FEATURES, axis=1)\n",
    "test_df = test_df.drop(LAG_FEATURES, axis=1)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rolling Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROLLING = [2, 4, 8]\n",
    "ROLLING_FEATURES = [f\"u_in_rolling_mean{w}\" for w in ROLLING]\n",
    "ROLLING_FEATURES += [f\"u_in_rolling_max{w}\" for w in ROLLING]\n",
    "ROLLING_FEATURES += [f\"u_in_rolling_min{w}\" for w in ROLLING]\n",
    "ROLLING_FEATURES += [f\"u_in_rolling_std{w}\" for w in ROLLING]\n",
    "    \n",
    "def add_rolling_features(df):\n",
    "    for w in ROLLING:\n",
    "        df[f\"u_in_rolling_mean{w}\"] = df[[\"breath_id\", \"u_in\"]].groupby(\"breath_id\").rolling(w).mean()[\"u_in\"].reset_index(drop=True)\n",
    "        df[f\"u_in_rolling_max{w}\"] = df[[\"breath_id\", \"u_in\"]].groupby(\"breath_id\").rolling(w).max()[\"u_in\"].reset_index(drop=True)\n",
    "        df[f\"u_in_rolling_min{w}\"] = df[[\"breath_id\", \"u_in\"]].groupby(\"breath_id\").rolling(w).min()[\"u_in\"].reset_index(drop=True)\n",
    "        df[f\"u_in_rolling_std{w}\"] = df[[\"breath_id\", \"u_in\"]].groupby(\"breath_id\").rolling(w).std()[\"u_in\"].reset_index(drop=True)\n",
    "    df = df.fillna(0)\n",
    "    return df\n",
    "\n",
    "train_df = add_rolling_features(train_df)\n",
    "test_df = add_rolling_features(test_df)\n",
    "\n",
    "train_df[ROLLING_FEATURES].to_feather(f\"../data/features/train_roll_v1.ftr\")\n",
    "test_df[ROLLING_FEATURES].to_feather(f\"../data/features/test_roll_v1.ftr\")\n",
    "\n",
    "train_df = train_df.drop(ROLLING_FEATURES, axis=1)\n",
    "test_df = test_df.drop(ROLLING_FEATURES, axis=1)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONT_FEATURES = ['u_in_cumsum', 'u_in_cummean', 'area', 'cross', 'cross2']\n",
    "\n",
    "def add_cont_feature(df):\n",
    "    df['time_delta'] = df.groupby('breath_id')['time_step'].diff().fillna(0)\n",
    "    df['delta'] = df['time_delta'] * df['u_in']\n",
    "    df['area'] = df.groupby('breath_id')['delta'].cumsum()\n",
    "\n",
    "    df['cross']= df['u_in']*df['u_out']\n",
    "    df['cross2']= df['time_step']*df['u_out']\n",
    "    \n",
    "    df['u_in_cumsum'] = (df['u_in']).groupby(df['breath_id']).cumsum()\n",
    "    df['one'] = 1\n",
    "    df['count'] = (df['one']).groupby(df['breath_id']).cumsum()\n",
    "    df['u_in_cummean'] =df['u_in_cumsum'] / df['count']\n",
    "    \n",
    "    df = df.drop(['count','one'], axis=1)\n",
    "    return df\n",
    "\n",
    "train_df = add_cont_feature(train_df)\n",
    "test_df = add_cont_feature(test_df)\n",
    "\n",
    "train_df[CONT_FEATURES].to_feather(f\"../data/features/train_cont_v1.ftr\")\n",
    "test_df[CONT_FEATURES].to_feather(f\"../data/features/test_cont_v1.ftr\")\n",
    "\n",
    "train_df = train_df.drop(CONT_FEATURES, axis=1)\n",
    "test_df = test_df.drop(CONT_FEATURES, axis=1)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract V1\n",
    "### Re-Load Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_features = [\"lag_v1\", \"cate_v1\", \"cont_v1\", \"roll_v1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORG_FEATURES = ['u_in', 'u_out', 'time_step']\n",
    "\n",
    "dfs = [train_df[ORG_FEATURES]]\n",
    "for fname in use_features:\n",
    "    _df = pd.read_feather(f\"../data/features/train_{fname}.ftr\")\n",
    "    dfs.append(_df)\n",
    "feat_train = pd.concat(dfs ,axis=1)\n",
    "\n",
    "dfs = [test_df[ORG_FEATURES]]\n",
    "for fname in use_features:\n",
    "    _df = pd.read_feather(f\"../data/features/test_{fname}.ftr\")\n",
    "    dfs.append(_df)\n",
    "feat_test = pd.concat(dfs ,axis=1)\n",
    "\n",
    "del dfs, _df\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NORM_FEATURES = CONT_FEATURES + LAG_FEATURES + ROLLING_FEATURES + ['u_in', 'time_step']\n",
    "\n",
    "def norm_scale(train_df, test_df):\n",
    "    scaler = RobustScaler()\n",
    "    all_u_in = np.vstack([train_df[NORM_FEATURES].values, test_df[NORM_FEATURES].values])\n",
    "    scaler.fit(all_u_in)\n",
    "    train_df[NORM_FEATURES] = scaler.transform(train_df[NORM_FEATURES].values)\n",
    "    test_df[NORM_FEATURES] = scaler.transform(test_df[NORM_FEATURES].values)\n",
    "    return train_df, test_df\n",
    "\n",
    "feat_train, feat_test = norm_scale(feat_train, feat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_train.to_feather(f\"../data/features/train_v1_all_norm.ftr\")\n",
    "feat_test.to_feather(f\"../data/features/test_v1_all_norm.ftr\")\n",
    "\n",
    "!ls ../data/features/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Lag Back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_LAG = 4\n",
    "LAG_BACK_FEATURES = [f'u_in_lag_{i}_back' for i in range(1, USE_LAG+1)]\n",
    "LAG_BACK_FEATURES += [f'u_in_time{i}_back' for i in range(1, USE_LAG+1)]\n",
    "LAG_BACK_FEATURES += [f'u_out_lag_{i}_back' for i in range(1, USE_LAG+1)]\n",
    "\n",
    "def add_lag_back_feature(df):\n",
    "    # https://www.kaggle.com/kensit/improvement-base-on-tensor-bidirect-lstm-0-173\n",
    "    for lag in range(1, USE_LAG+1):\n",
    "        df[f'breath_id_lag{lag}_back']=df['breath_id'].shift(-lag).fillna(0)\n",
    "        df[f'breath_id_lag{lag}same_back']=np.select([df[f'breath_id_lag{lag}_back']==df['breath_id']], [1], 0)\n",
    "\n",
    "        # u_in \n",
    "        df[f'u_in_lag_{lag}_back'] = df['u_in'].shift(-lag).fillna(0) * df[f'breath_id_lag{lag}same_back']\n",
    "        df[f'u_in_time{lag}_back'] = df['u_in'] - df[f'u_in_lag_{lag}_back']\n",
    "        df[f'u_out_lag_{lag}_back'] = df['u_out'].shift(-lag).fillna(0) * df[f'breath_id_lag{lag}same_back']\n",
    "\n",
    "    drop_columns = [f'breath_id_lag{i}_back' for i in range(1, USE_LAG+1)]\n",
    "    drop_columns += [f'breath_id_lag{i}same_back' for i in range(1, USE_LAG+1)]\n",
    "    df = df.drop(drop_columns, axis=1)\n",
    "\n",
    "    # fill na by zero\n",
    "    df = df.fillna(0)\n",
    "    return df\n",
    "\n",
    "train_df = add_lag_back_feature(train_df)\n",
    "test_df = add_lag_back_feature(test_df)\n",
    "\n",
    "train_df[LAG_BACK_FEATURES].to_feather(f\"../data/features/train_lag_back_v1.ftr\")\n",
    "test_df[LAG_BACK_FEATURES].to_feather(f\"../data/features/test_lag_back_v1.ftr\")\n",
    "\n",
    "train_df = train_df.drop(LAG_BACK_FEATURES, axis=1)\n",
    "test_df = test_df.drop(LAG_BACK_FEATURES, axis=1)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous Features V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONT_FEATURES_V2 = ['u_in_mean', 'u_in_std', 'breath_id_u_out_mean', 'last_value_u_in', 'first_value_u_in']\n",
    "\n",
    "def add_cont_feature_v2(df):\n",
    "    u_in_mean_dict = df.groupby('breath_id')['u_in'].mean().to_dict()\n",
    "    df['u_in_mean'] = df['breath_id'].map(u_in_mean_dict)\n",
    "    del u_in_mean_dict\n",
    "    u_in_std_dict = df.groupby('breath_id')['u_in'].std().to_dict()\n",
    "    df['u_in_std'] = df['breath_id'].map(u_in_std_dict)\n",
    "    del u_in_std_dict\n",
    "\n",
    "    df['breath_id_u_out_mean'] =df.groupby(['breath_id'])['u_out'].sum()\n",
    "\n",
    "    df['last_value_u_in'] = df.groupby('breath_id')['u_in'].transform('last')\n",
    "    df['first_value_u_in'] = df.groupby('breath_id')['u_in'].transform('first')\n",
    "    \n",
    "    return df\n",
    "\n",
    "train_df = add_cont_feature_v2(train_df)\n",
    "test_df = add_cont_feature_v2(test_df)\n",
    "\n",
    "train_df[CONT_FEATURES_V2].to_feather(f\"../data/features/train_cont_v2.ftr\")\n",
    "test_df[CONT_FEATURES_V2].to_feather(f\"../data/features/test_cont_v2.ftr\")\n",
    "\n",
    "train_df = train_df.drop(CONT_FEATURES_V2, axis=1)\n",
    "test_df = test_df.drop(CONT_FEATURES_V2, axis=1)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EWM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EWM_LST = [8, 16, 32]\n",
    "EWM_FEATURES = [f'ewm_u_in_mean_{i}' for i in EWM_LST]\n",
    "EWM_FEATURES += [f'ewm_u_in_std_{i}' for i in EWM_LST]\n",
    "EWM_FEATURES += [f'ewm_u_in_corr_{i}' for i in EWM_LST]\n",
    "\n",
    "def add_ewm_feature(df):\n",
    "    for e in EWM_LST:\n",
    "        df[f'ewm_u_in_mean_{e}'] = df.groupby('breath_id')['u_in'].ewm(halflife=8).mean().reset_index(level=0,drop=True)\n",
    "        df[f'ewm_u_in_std_{e}'] = df.groupby('breath_id')['u_in'].ewm(halflife=16).std().reset_index(level=0,drop=True) \n",
    "        df[f'ewm_u_in_corr_{e}'] = df.groupby('breath_id')['u_in'].ewm(halflife=32).corr().reset_index(level=0,drop=True) \n",
    "    return df\n",
    "\n",
    "train_df = add_ewm_feature(train_df)\n",
    "test_df = add_ewm_feature(test_df)\n",
    "\n",
    "train_df[EWM_FEATURES].to_feather(f\"../data/features/train_ewm_v1.ftr\")\n",
    "test_df[EWM_FEATURES].to_feather(f\"../data/features/test_ewm_v1.ftr\")\n",
    "\n",
    "train_df = train_df.drop(EWM_FEATURES, axis=1)\n",
    "test_df = test_df.drop(EWM_FEATURES, axis=1)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_features = [\"lag_back_v1\", \"cont_v2\", \"ewm_v1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for fname in use_features:\n",
    "    _df = pd.read_feather(f\"../data/features/train_{fname}.ftr\")\n",
    "    dfs.append(_df)\n",
    "feat_train_v2 = pd.concat(dfs ,axis=1)\n",
    "\n",
    "dfs = []\n",
    "for fname in use_features:\n",
    "    _df = pd.read_feather(f\"../data/features/test_{fname}.ftr\")\n",
    "    dfs.append(_df)\n",
    "feat_test_v2 = pd.concat(dfs ,axis=1)\n",
    "\n",
    "del dfs, _df\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normarize V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NORM_FEATURES_V2 = LAG_BACK_FEATURES + CONT_FEATURES_V2 + EWM_FEATURES\n",
    "\n",
    "def norm_scale_v2(train_df, test_df):\n",
    "    scaler = RobustScaler()\n",
    "    all_u_in = np.vstack([train_df[NORM_FEATURES_V2].values, test_df[NORM_FEATURES_V2].values])\n",
    "    scaler.fit(all_u_in)\n",
    "    train_df[NORM_FEATURES_V2] = scaler.transform(train_df[NORM_FEATURES_V2].values)\n",
    "    test_df[NORM_FEATURES_V2] = scaler.transform(test_df[NORM_FEATURES_V2].values)\n",
    "    return train_df, test_df\n",
    "\n",
    "feat_train_v2, feat_test_v2 = norm_scale_v2(feat_train_v2, feat_test_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_train_v2.to_feather(f\"../data/features/train_v2_all_norm.ftr\")\n",
    "feat_test_v2.to_feather(f\"../data/features/test_v2_all_norm.ftr\")\n",
    "\n",
    "!ls ../data/features/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## target encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = {}\n",
    "for r, df in train_df.groupby('R'):\n",
    "    d = {\n",
    "         f\"R_mean\": df['pressure'].mean(),\n",
    "         f\"R_std\": df['pressure'].std(),\n",
    "         f\"R_mean_u_out0\": df.query('u_out==0')['pressure'].mean(),\n",
    "         f\"R_std_u_out0\": df.query('u_out==0')['pressure'].std(),\n",
    "    }\n",
    "    feat[r] = d\n",
    "train_target_r = pd.DataFrame(train_df['R'].map(feat).tolist())\n",
    "test_target_r = pd.DataFrame(test_df['R'].map(feat).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = {}\n",
    "for c, df in train_df.groupby('C'):\n",
    "    d = {\n",
    "         f\"C_mean\": df['pressure'].mean(),\n",
    "         f\"C_std\": df['pressure'].std(),\n",
    "         f\"C_mean_u_out0\": df.query('u_out==0')['pressure'].mean(),\n",
    "         f\"C_std_u_out0\": df.query('u_out==0')['pressure'].std(),\n",
    "    }\n",
    "    feat[c] = d\n",
    "train_target_c = pd.DataFrame(train_df['C'].map(feat).tolist())\n",
    "test_target_c = pd.DataFrame(test_df['C'].map(feat).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['R_C'] = train_df.apply(lambda x: f\"{x.R}_{x.C}\", axis=1)\n",
    "test_df['R_C'] = test_df.apply(lambda x: f\"{x.R}_{x.C}\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = {}\n",
    "for rc, df in train_df.groupby(['R_C']):\n",
    "    d = {\n",
    "         f\"RC_mean\": df['pressure'].mean(),\n",
    "         f\"RC_std\": df['pressure'].std(),\n",
    "         f\"RC_mean_u_out0\": df.query('u_out==0')['pressure'].mean(),\n",
    "         f\"RC_std_u_out0\": df.query('u_out==0')['pressure'].std(),\n",
    "    }\n",
    "    feat[rc] = d\n",
    "train_target_rc = pd.DataFrame(train_df['R_C'].map(feat).tolist())\n",
    "test_target_rc = pd.DataFrame(test_df['R_C'].map(feat).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target_features = pd.concat([train_target_r, train_target_c, train_target_rc], axis=1)\n",
    "test_target_features = pd.concat([test_target_r, test_target_c, test_target_rc], axis=1)\n",
    "\n",
    "train_target_features.to_feather(f\"../data/features/train_target_feat_v1.ftr\")\n",
    "test_target_features.to_feather(f\"../data/features/test_target_feat_v1.ftr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target_features.to_feather(f\"../data/features/train_target_feat_v1.ftr\")\n",
    "test_target_features.to_feather(f\"../data/features/test_target_feat_v1.ftr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_FEATURES = [\n",
    "    'R_mean', 'R_std', 'R_mean_u_out0', 'R_std_u_out0',\n",
    "    'C_mean', 'C_std', 'C_mean_u_out0', 'C_std_u_out0',\n",
    "    'RC_mean', 'RC_std', 'RC_mean_u_out0', 'RC_std_u_out0'\n",
    "]\n",
    "\n",
    "def norm_scale_v3(train_df, test_df):\n",
    "    scaler = RobustScaler()\n",
    "    all_u_in = np.vstack([train_df[TARGET_FEATURES].values, test_df[TARGET_FEATURES].values])\n",
    "    scaler.fit(all_u_in)\n",
    "    train_df[TARGET_FEATURES] = scaler.transform(train_df[TARGET_FEATURES].values)\n",
    "    test_df[TARGET_FEATURES] = scaler.transform(test_df[TARGET_FEATURES].values)\n",
    "    return train_df, test_df\n",
    "\n",
    "feat_train_v3, feat_test_v3 = norm_scale_v3(train_target_features, test_target_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_train_v3.to_feather(f\"../data/features/train_v3_all_norm.ftr\")\n",
    "feat_test_v3.to_feather(f\"../data/features/test_v3_all_norm.ftr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
